{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning part\n",
    "### This cell loads the data and normalizes it to the [0,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "\n",
    "Nc=10\n",
    "N=784\n",
    "Ns=60000\n",
    "M=np.zeros((0,N))\n",
    "for i in range(Nc):\n",
    "    M=np.concatenate((M, mat['train'+str(i)]), axis=0)\n",
    "M=M/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a heatmap of the weights a helper function is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky, yy=np.random.randint(1800)):\n",
    "    #yy=0\n",
    "    HM=np.zeros((28*Ky,28*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: `eps0` - initial learning rate that is linearly annealed during training; `hid` - number of hidden units that are displayed as an `Ky` by `Kx` array by the helper function defined above; `mu` - the mean of the gaussian distribution that initializes the weights; `sigma` - the standard deviation of that gaussian; `Nep` - number of epochs; `Num` - size of the minibatch; `prec` - parameter that controls numerical precision of the weight updates; `delta` - the strength of the anti-hebbian learning; `p` - Lebesgue norm of the weights; `k` - ranking parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0=0.04    # learning rate\n",
    "Kx=10\n",
    "Ky=10\n",
    "hid=2000 #Kx*Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "mu=0.0\n",
    "sigma=1.0\n",
    "Nep=1000      # number of epochs\n",
    "Num=100      # size of the minibatch\n",
    "prec=1e-30\n",
    "delta=0.4    # Strength of the anti-hebbian learning\n",
    "p=3.0        # Lebesgue norm of the weights\n",
    "k=7          # ranking parameter, must be integer that is bigger or equal than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "\n",
    "synapses = np.random.normal(mu, sigma, (hid, N))\n",
    "for nep in range(Nep):\n",
    "    print(nep)\n",
    "    eps=eps0*(1-nep/Nep)\n",
    "    M=M[np.random.permutation(Ns),:]\n",
    "    for i in range(Ns//Num):\n",
    "        inputs=np.transpose(M[i*Num:(i+1)*Num,:])\n",
    "        sig=np.sign(synapses)\n",
    "        tot_input=np.dot(sig*np.absolute(synapses)**(p-1),inputs)\n",
    "        \n",
    "        y=np.argsort(tot_input,axis=0)\n",
    "        yl=np.zeros((hid,Num))\n",
    "        yl[y[hid-1,:],np.arange(Num)]=1.0\n",
    "        yl[y[hid-k],np.arange(Num)]=-delta\n",
    "        \n",
    "        xx=np.sum(np.multiply(yl,tot_input),1)\n",
    "        ds=np.dot(yl,np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1),(1,N)),synapses)\n",
    "        \n",
    "        nc=np.amax(np.absolute(ds))\n",
    "        if nc<prec:\n",
    "            nc=prec\n",
    "        synapses += eps*np.true_divide(ds,nc)\n",
    "        \n",
    "    draw_weights(synapses, Kx, Ky)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./synapses\", synapses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize neurons of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "synapses = np.load(\"./synapses.npy\")\n",
    "draw_weights(synapses, 10, 10, yy=np.random.randint(1800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning part\n",
    "\n",
    "### Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras as keras \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "#y_train = one_hot(y_train)\n",
    "#y_test = one_hot(y_test)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2000, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "sgd = keras.optimizers.Adam(lr=0.0002,) #decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=15, min_lr=0.000001)\n",
    "#early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                              patience=3,)\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "logdir = \"logs/end_to_end/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=300,\n",
    "          steps_per_epoch=60000 // 100,\n",
    "          validation_data=(x_test, y_test),\n",
    "          validation_steps=10000 // 100,verbose=1,\n",
    "          callbacks=[reduce_lr, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bio learning model - training output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow.keras as keras \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "synapses = np.load(\"./synapses.npy\")\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_activation(x):\n",
    "\n",
    "    zeros = tf.zeros(tf.shape(x), dtype=x.dtype.base_dtype)\n",
    "    out = tf.math.pow(x, 1.)\n",
    "\n",
    "    def grad(dy):\n",
    "        return dy\n",
    "    return keras.backend.switch(x > 0.0, out, zeros), grad\n",
    "\n",
    "def loss(labels, pred):\n",
    "    return tf.math.reduce_mean(tf.math.pow(tf.abs(tf.math.subtract(pred, labels)), 6.))\n",
    "\n",
    "def one_hot(arr, nb_classes = 10):\n",
    "    targets = np.array(arr).reshape(-1)\n",
    "    one_hot_targets = np.eye(nb_classes)[targets]\n",
    "    one_hot_targets[one_hot_targets == 0] = -1\n",
    "    return one_hot_targets\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "#y_train = one_hot(y_train)\n",
    "#y_test = one_hot(y_test)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(synapses.shape[0], input_dim=synapses.shape[1], activation=custom_activation, trainable=False))\n",
    "model.layers[0].set_weights([np.transpose(synapses), np.zeros((synapses.shape[0]))])\n",
    "model.add(Dense(10, activation='softmax', trainable=True))\n",
    "\n",
    "sgd = keras.optimizers.Adam(lr=0.0002,) #decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=loss,\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=15, min_lr=0.000001)\n",
    "\n",
    "#score = model.evaluate(x_test, y_test, verbose=0)\n",
    "logdir = \"logs/bio/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=300,\n",
    "          steps_per_epoch=60000 // 100,\n",
    "          validation_data=(x_test, y_test),\n",
    "          validation_steps=10000 // 100,verbose=1, \n",
    "          callbacks=[tensorboard_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
